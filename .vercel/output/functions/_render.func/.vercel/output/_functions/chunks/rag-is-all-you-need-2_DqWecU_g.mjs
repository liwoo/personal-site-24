const id = "rag-is-all-you-need-2.mdx";
						const collection = "post";
						const slug = "rag-is-all-you-need-2";
						const body = "import { Image } from 'astro:assets';\nimport vector from '~/assets/images/rag/vector-space.svg';\nimport pipeline from '~/assets/images/rag/rag-pipeline.svg';\n\n---\n\n_This is the second part of our \"RAG is all you need\" series, where we're exploring the nuts and bolts of Retrieval Augmented Generation. If you haven't checked out [Part 1](/rag-is-all-you-need) where we covered the fundamentals of how LLMs work and why grounding them to some known truth is essential in modern AI engineering, definitely give that a read first!_\n\n---\n## Introduction\n\nSo last time we talked about why RAG is such a game-changer for AI applications, right? Like, it's literally becoming one of those must-have tools in every AI engineer's toolkit. Today though? We're gonna deep dive into something super cool - vector search. And trust me, once you get this, a lot of things about modern AI just click! üîç\n\nLet me start with something that's been bugging computer scientists for ages: how do we get computers to actually understand language the way we do? Here's a fun little exercise - think about these words: \n\n- table üõèÔ∏è\n- flower üå∫\n- lamp ü™î\n- basketball üèÄ\n\nNow, if I throw in the word \"chair\" and ask you which one's most related... you'd probably say table, right?\n\nBut here's where it gets interesting! If I asked you why, you might start talking about how chairs and tables go together during meals, or how they're both made of wood. But wait - basketballs boards are made of wood too! And technically, you sit near lamps all the time... See how messy this gets? üòÖ\n\n<Image\n  src={vector}\n  alt=\"Vector Illustration of embedded words\"\n  height={400}\n/>\n\nNLP researchers have been cooking up better and better ways to handle these nuanced relationships. They started with pretty basic stuff like [bag-of-words](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) (kinda naive, if you ask me), but then [BERT](https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model) came along and... boom!üí•\n\nWhat BERT and similar models do is super clever - they map words into this dense vector space where relationships are represented by how words appear together in massive amounts of text. It's like they're creating this multidimensional map of language itself! And when I say multidimensional, I mean like... way more than the 2D diagram above. Think 512 dimensions or more!\n\nLet me try and break it down like this: every word gets its own unique \"location\" in this space, and words that often show up together in similar contexts end up closer to each other. Modern language models like Claude and GPT have gotten perfect at this mapping!\n\n## Understanding Vector Embeddings in Practice\n\nWhen it comes to actually building AI applications (isn't that why we are all here üòÖ), we need to take our knowledge base - our ground truth - and convert it into these vector embeddings. This is where we begin to differentiate our AI Chatbot from the rest, the uniqueness and usefulness of the data you want to make available:\n- Company documentation\n- School syllabi\n- Performance reports\n- Pretty much any text you want your AI to understand!\n\n\nToday, there's several approaches you could take in creating these embeddings. Like, sure, you could use smaller models like BERT, and honestly? If you're just getting started or working on a smaller project, that's totally fine! You can do it absolutely free with these open-source BERT models.\n\nBut in production, we usually reach for these high-dimension, high-accuracy embedding models. I'm talking about models from OpenAI, for example. Why? Because they just handle those tricky, ambiguous language tasks so much better!\n\n```python\nfrom langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport os\nfrom typing import List, Dict\n\nclass EmbeddingGenerator:\n    def __init__(self, model_type: str = \"openai\"):\n        \"\"\"\n        Initialize our embedding generator with either OpenAI or a local BERT model\n        \"\"\"\n        if model_type == \"openai\":\n            self.embeddings = OpenAIEmbeddings()\n        else:\n            # Using a smaller BERT model - great for testing!\n            self.embeddings = HuggingFaceEmbeddings(\n                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n            )\n        \n        # This is where the magic happens with text splitting\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200,  # A bit of overlap helps maintain context\n            length_function=len,\n        )\n\n    async def generate_embeddings(self, text: str) -> List[Dict]:\n        \"\"\"\n        Take our text, split it into chunks, and generate embeddings\n        \"\"\"\n        # First, let's split our text into manageable chunks\n        chunks = self.text_splitter.split_text(text)\n        \n        # Now generate embeddings for each chunk\n        embeddings = []\n        for chunk in chunks:\n            vector = await self.embeddings.aembed_query(chunk)\n            embeddings.append({\n                \"text\": chunk,\n                \"vector\": vector,\n                \"metadata\": {\n                    \"length\": len(chunk),\n                    # You might want to add more metadata here\n                }\n            })\n        \n        return embeddings\n\n# Let's see it in action!\nasync def main():\n    # Create our generator\n    generator = EmbeddingGenerator(model_type=\"openai\")\n    \n    # Some sample text\n    text = \"\"\"\n    Vector search is a fascinating technique in AI. \n    It allows us to find similar content by converting text into \n    high-dimensional vectors and comparing them using cosine similarity.\n    \"\"\"\n    \n    # Generate those embeddings!\n    embeddings = await generator.generate_embeddings(text)\n    print(f\"Generated {len(embeddings)} embeddings!\")\n    print(f\"First embedding vector size: {len(embeddings[0]['vector'])}\")\n```\n\n## The Vector Database Game\n\nOkay, so you and I know all about Relational Database Management Systems (RDBMSs) right, but they're really built for storing stuff in rows and columns - very 2D thinking. Some databases get a bit fancier with graphs or nodes (looking at you, [Neo4j!](https://neo4j.com/)), but vector databases? They're built different!\n\nThese specialized databases can handle information in vector space that's super multidimensional. We're talking hundreds or even thousands of dimensions! And over the last few years, we've seen some really cool options pop up:\n\n- [Pinecone](https://www.pinecone.io/) (built specifically for vector search)\n- [Chroma](https://www.trychroma.com/) (built specifically for vector search)\n- PostgreSQL with [pg_vector](https://github.com/pgvector/pgvector) (for when you want to stick with the classics)\n- [Redis](https://redis.io/solutions/vector-database/) (my personal favorite because... performance! üöÄ)\n\n![Benchmark of Vector Db Performance](https://blueteam.ai/blog/vector-benchmarking/Zilliz_Test_Example.png)\n\nYou can run a comparison yourself [here](https://github.com/zilliztech/VectorDBBench).\n\n## Building The Pipeline\n\nSo here's how the whole thing actually works in practice. When someone asks a question, we:\n\n1. Store your ground truth in a vector database\n2. Convert the user's question into an embedding\n3. Use cosine similarity (which is actually a pretty simple algorithm) to find matches\n4. Pull back maybe the top 10 most relevant chunks of information\n\n<Image\n  src={pipeline}\n  alt=\"RAG Pipeline illustration\"\n  height={400}\n/>\n\nNow, there's actually a whole science to storing these embeddings! It depends on stuff like:\n- Which LLM you're using\n- The context window size you're working with\n- How much data you can throw at it before things get wonky\n\nBut honestly, these newer models with their huge context windows? They've made this SO much easier! You can throw a pretty big chunk of context at them, and they handle it like champs. No more worrying about degrading quality or hallucinations (well, mostly! üòÖ).\n\n## Practical Implementation Tips\n\nWhen you're actually implementing this, you want to:\n1. Chunk your data into manageable sizes\n2. Store references to your original documents\n3. Fetch those reference docs when you get a match\n\nThis way, when your LLM is answering questions, it's not just pulling from its training data - it's using actual, accurate information from your knowledge base!\n\nBelow is a representation of how you could your RAG data using Postgres\n\n#### documents\n| Column     | Type                     | Description                                |\n|------------|--------------------------|-------------------------------------------|\n| id         | UUID                     | Primary key - unique document identifier   |\n| title      | TEXT                     | Document title/name                       |\n| source_url | TEXT                     | Where this doc came from (optional)       |\n| created_at | TIMESTAMP WITH TIMEZONE  | When we first added this                  |\n| updated_at | TIMESTAMP WITH TIMEZONE  | Last time we touched this                 |\n\n#### document_chunks\n| Column      | Type                     | Description                               |\n|-------------|--------------------------|-------------------------------------------|\n| id          | UUID                     | Primary key - unique chunk identifier     |\n| document_id | UUID                     | Links back to our main document üîó        |\n| chunk_index | INTEGER                  | Helps us keep chunks in order (super important!) |\n| content     | TEXT                     | The actual chunk of text                  |\n| embedding   | vector(1536)             | Our fancy vector embedding - magic! ‚ú®     |\n| metadata    | JSONB                    | Flexible metadata storage (love this!)    |\n| tokens      | INTEGER                  | Token count (helpful for context windows) |\n| chunk_size  | INTEGER                  | Size of this chunk in chars              |\n| created_at  | TIMESTAMP WITH TIMEZONE  | When we created this chunk               |\n\nYou know what's cool about this setup? The `metadata` JSONB field is like this super flexible catch-all for whatever extra info you might need to track. This can become helpful for improving the relevance and accuracy of your search (more on that later on in the series). I usually stuff things in there like:\n\n```json\n{\n    \"source_type\": \"technical_doc\",\n    \"department\": \"engineering\",\n    \"last_verified\": \"2024-01-18\",\n    \"confidence\": 0.95,\n    \"page_number\": 42,\n    \"section\": \"API Documentation\"\n}\n```\n___\n\nNext up, we'll roll up our sleeves and implement a real world RAG solution against the [MANEB syllabus](https://www.maneb.edu.mw/) - which can be used later to create a context aware AI tutor for Malawian Students! \n";
						const data = {publishDate:new Date(1726012800000),draft:false,title:"RAG is all you need - Part 2",excerpt:"Ever stared at your computer screen, wondering how the heck these AI models actually understand what we're saying? Like, how does ChatGPT know that when I say chair, it's more related to table than basketball? Well, buckle up, because we're about to dive into one of the coolest parts of modern AI engineering - vector search! üöÄ",image:"https://minio-api.dartsmw.com/chienda.com/political-rag.png",category:"Tech",tags:["rag","llm","embeddings","ai"]};
						const _internal = {
							type: 'content',
							filePath: "/Users/liwu/Projects/websites/personal-24/src/content/post/rag-is-all-you-need-2.mdx",
							rawData: undefined,
						};

export { _internal, body, collection, data, id, slug };
